## ---------------------------------------------- ##
           # WRTDS Centralized Workflow
## ---------------------------------------------- ##
# WRTDS = Weighted Regressions on Time, Discharge, and Season
## Nick J Lyon

## ---------------------------------------------- ##
                # Housekeeping ----
## ---------------------------------------------- ##
# Load libraries
# install.packages("librarian")
librarian::shelf(tidyverse, googledrive, lubridate, EGRET, EGRETci, lter/HERON, supportR)

# Clear environment
rm(list = ls())

# If working on server, need to specify correct path
(path <- scicomptools::wd_loc(local = FALSE, remote_path = file.path('/', "home", "shares", "lter-si", "WRTDS")))

# Make sure necessary folders exist
dir.create(path = file.path(path, "WRTDS Inputs"), showWarnings = F)
dir.create(path = file.path(path, "WRTDS Temporary Files"), showWarnings = F)
dir.create(path = file.path(path, "WRTDS Loop Diagnostic"), showWarnings = F)
dir.create(path = file.path(path, "WRTDS Outputs"), showWarnings = F)

# Identify CSVs generated by 'step-2' script
input_ids <- googledrive::drive_ls(googledrive::as_id("https://drive.google.com/drive/u/0/folders/1QEofxLdbWWLwkOTzNRhI6aorg7-2S3JE"))

# Download them locally
purrr::walk2(.x = input_ids$name, .y = input_ids$id,
             .f = ~ googledrive::drive_download(file = .y, overwrite = T,
                                                path = file.path(path, "WRTDS Inputs", .x)))

# Read in CSVs generated by 'step-2' script
discharge <- read.csv(file.path(path, "WRTDS Inputs", "WRTDS-input_discharge.csv"))
chemistry <- read.csv(file.path(path, "WRTDS Inputs", "WRTDS-input_chemistry.csv"))
information <- read.csv(file.path(path, "WRTDS Inputs", "WRTDS-input_information.csv"))

## ---------------------------------------------- ##
            # Diagnose Types of Sites ----
## ---------------------------------------------- ##

# Rivers with warning that there are duplicated dates (should be impossible)
duplicate_data <- c(
  # Warning about "duplicated Daily dates" and "duplicated Sample dates"
  ### "Error in seq.Date(surfaceStart, by = "1 year", length.out = nSeg) : 
  ###'from' must be of length 1"
  "USGS__Wild River_DSi", "USGS__Wild River_NH4", "USGS__Wild River_NOx", 
  "USGS__Wild River_P"
  )

pa5_5 <- c(
  # EGRET::setPA(eList = egret_list[_out], paStart = 5, paLong = 5)
  "NWT__MARTINELLI_DSi", "NWT__MARTINELLI_NH4", 
  "NWT__MARTINELLI_NOx", "NWT__MARTINELLI_P")

pa5_3 <- c(
  # EGRET::setPA(eList = egret_list[_out], paStart = 5, paLong = 3)
  "NWT__SADDLE STREAM 007_DSi", "NWT__SADDLE STREAM 007_NH4", 
  "NWT__SADDLE STREAM 007_NOx", "NWT__SADDLE STREAM 007_P")

# Rivers without matching chemistry/discharge data
missing_data <- c(
  # Warning in `EGRET::mergeReport`
  ### "Some Sample dates do not have corresponding flow data. Not all EGRET functions will work correctly."
  "LUQ__RI_DSi", "LUQ__RI_NH4", "LUQ__RI_NOx", "LUQ__RI_P"
  )

# Other odd errors
odd_ones <- c(
  # "Error in seq.default(xFirst, xLast) : 'from' must be a finite number"
  ### Looks like this may be caused by "negative flow days"?
  ### Need to check range of values to see these problem values
  "USGS__Lower Atchafalaya_DSi", "USGS__Lower Atchafalaya_NH4", 
  "USGS__Lower Atchafalaya_NOx", "USGS__Lower Atchafalaya_P",
  "USGS__YAZOO RIVER_DSi", "USGS__YAZOO RIVER_NH4",
  "USGS__YAZOO RIVER_NOx", "USGS__YAZOO RIVER_P",
  ## "Error in if (lastMonth == 2 & (lastYear%%4 == 0) & ((lastYear%%100 !=  : missing value where TRUE/FALSE needed"
  "Australia__DARLING RIVER AT BOURKE TOWN_NO3",
  "Australia__DARLING RIVER AT BURTUNDY_NO3",
  "Australia__DARLING RIVER AT BURTUNDY_NOx",
  "Australia__DARLING RIVER AT BURTUNDY_P",
  "Australia__DARLING RIVER AT WILCANNIA MAIN CHANNEL_NO3",
  "Australia__EDWARD RIVER AT MOULAMEIN_NOx",
  "Cameroon__Nsimi_outlet_DSi", "Cameroon__Nsimi_outlet_NO3",
  "HYBAM__Borja_DSi", "HYBAM__Itaituba_DSi", "HYBAM__Itaituba_NO3",
  "HYBAM__Langa Tabiki_DSi", "HYBAM__Langa Tabiki_NO3", 
  ## "Error in if (good) { : missing value where TRUE/FALSE needed"
  "HYBAM__Manacapuru_DSi",
  # Crashes R without a specific warning message
  "USGS__GREEN RIVER_P", "USGS__McDonalds Branch_P", "USGS__MERCED R_P",
  "USGS__PINE CREEK_P", "USGS__SOPCHOPPY RIVER_NOx",
  "Australia__EDWARD RIVER AT DENILIQUIN_NOx",
  "Australia__PEEL RIVER AT UPSTREAM PARADISE WEIR_NO3"
)

# Rivers without sufficient data
few_data <- c(
  # Error in `EGRET::modelEstimation`
  ## "Error in runSurvReg(SampleCrossV$DecYear[i], SampleCrossV$LogQ[i], DecLow,  : minNumUncen is greater than total number of samples"
  ## Note: error timing is 1-3 minutes despite `modelEstimation` being an early function
  "AND__GSWS06_NOx", "MCM__Priscu Stream at B1_NH4", "MCM__Von Guerard Stream at F6_NH4",
  "MCM__Von Guerard Stream at F6_NH4", "NIVA__VESENUM_P", "NIVA__OSTEGLO_NH4",
  "Australia__BARWON RIVER AT DANGAR BRIDGE WALGETT_NOx",
  "Australia__BARWON RIVER AT DANGAR BRIDGE WALGETT_P",
  "Australia__BILLABONG CREEK AT DARLOT_P",
  "Australia__DARLING RIVER AT BOURKE TOWN_NH4",
  "Australia__DARLING RIVER AT BOURKE TOWN_NOx",
  "Australia__DARLING RIVER AT BOURKE TOWN_P",
  "Australia__DARLING RIVER AT WILCANNIA MAIN CHANNEL_NH4",
  "Australia__DARLING RIVER AT WILCANNIA MAIN CHANNEL_NOx",
  "Australia__DARLING RIVER AT WILCANNIA MAIN CHANNEL_P",
  "Australia__EDWARD RIVER AT DENILIQUIN_NH4",
  "Australia__EDWARD RIVER AT DENILIQUIN_NO3",
  "Australia__NARRABRI CREEK AT NARRABRI_NH4",
  "Australia__PEEL RIVER AT UPSTREAM PARADISE WEIR_P", "HYBAM__Atalaya Aval_NO3",
  # Error in `EGRET::runSeries`
  ## "Error in runSurvReg(estPtYear, estPtLogQ, DecLow, DecHigh, localSample,  : minNumObs is greater than total number of samples"
  ## Note: error timing is near-instant
  "AND__GSWS07_NOx", "GRO__Kolyma_P", "GRO__Lena_P", "GRO__Mackenzie_P", "GRO__Yukon_P",
  "HBR__ws1_P", "HBR__ws2_P", "HBR__ws3_P", "HBR__ws4_P", "HBR__ws5_P",
  "HBR__ws6_P", "HBR__ws7_P", "HBR__ws8_P", "HBR__ws9_P",
  "MCM__Canada Stream at F1_P", "MCM__Onyx River at Lake Vanda Weir_NH4",
  "MCM__Onyx River at Lake Vanda Weir_P", "MCM__Onyx River at Lower Wright Weir_P",
  "NIVA__AAGEVEG_DSi", "NIVA__BUSEDRA_NH4", "NIVA__BUSEDRA_P", "NIVA__FINEALT_NH4",
  "NIVA__FINEALT_P", "NIVA__FINEPAS_DSi", "NIVA__FINETAN_DSi", "NIVA__HOREVOS_DSi",
  "NIVA__MROEDRI_DSi", "NIVA__NOREVEF_NH4", "NIVA__NOREVEF_P", "NIVA__OSLEALN_DSi",
  "NIVA__ROGEBJE_DSi", "NIVA__ROGEVIK_DSi", "NIVA__SFJENAU_DSi", "NIVA__STRENID_DSi",
  "NIVA__STREORK_NH4", "NIVA__STREORK_P", "NIVA__TELESKI_NH4", "NIVA__TELESKI_P",
  "NIVA__VAGEOTR_NH4", "NIVA__VAGEOTR_P", "NIVA__VESENUM_NH4", "NWT__ALBION_NOx",
  "NWT__ALBION_P", "NWT__MARTINELLI_P", "NWT__SADDLE STREAM 007_NOx",
  "NWT__SADDLE STREAM 007_P", "Sagehen__Sagehen_NH4", "USGS__ANDREWS CREEK_NH4",
  "USGS__ANDREWS CREEK_P", "USGS__SOUTH PLATTE_NH4", "USGS__SOUTH PLATTE_NOx",
  "USGS__SOUTH PLATTE_P", "Australia__BARWON RIVER AT MUNGINDI_NO3",
  "Australia__BARWON RIVER AT MUNGINDI_P", "Australia__BILLABONG CREEK AT DARLOT_NO3",
  "Australia__BILLABONG CREEK AT DARLOT_NOx",
  "Australia__DARLING RIVER AT BURTUNDY_NH4",
  "Australia__DARLING RIVER AT MENINDEE UPSTREAM WEIR 32_NH4",
  "Australia__EDWARD RIVER AT DENILIQUIN_P",
  "Australia__EDWARD RIVER AT MOULAMEIN_P",
  "Australia__MURRAY RIVER DOWNSTREAM YARRAWONGA WEIR_NH4",
  "Australia__NAMOI RIVER AT GOANGRA_NH4",
  "Australia__NAMOI RIVER AT GOANGRA_NOx",
  "Australia__NAMOI RIVER AT GOANGRA_P",
  "Australia__NARRABRI CREEK AT NARRABRI_NOx",
  "Australia__NARRABRI CREEK AT NARRABRI_P",
  "Australia__PEEL RIVER AT UPSTREAM PARADISE WEIR_NH4",
  "HYBAM__Borja_NO3", "HYBAM__Ciudad Bolivar_NO3"
  )

# Identify all rivers that aren't in the broken data vectors
good_rivers <- setdiff(x = unique(chemistry$Stream_Element_ID),
                       y = unique(c(missing_data, few_data, duplicate_data, odd_ones)))
## Note this includes weird rivers that need special treatment and those that don't

## ---------------------------------------------- ##
              # Analysis Workflow ----
## ---------------------------------------------- ##
# Set of rivers we've already run the workflow for
done_rivers <- data.frame("file" = dir(path = file.path(path, "WRTDS Loop Diagnostic"))) %>%
  # Drop the file suffix part of the file name 
  dplyr::mutate(river = gsub(pattern = "\\_Loop\\_Diagnostic.csv", replacement = "", x = file))

# Set of problem rivers to drop from the loop
bad_rivers <- c( )

# Identify rivers to run
rivers_to_do <- setdiff(x = unique(good_rivers), 
                        y = c(unique(done_rivers$river), bad_rivers))

# What are the next few that will be processed?
rivers_to_do[1:5]

# Kalman step throws the following error:
## "Pseudo only supported after running modelEstimation
## Please double check that the Sample dataframe is correctly defined.
## Missing columns:ConcLowConcHighConcAve
## Error in `[.data.frame`(localSample, , c("Julian", "ConcAve")) : 
##  undefined columns selected"

# Loop across rivers and elements to run WRTDS workflow!
for(river in rivers_to_do){ # actual loop
  # for(river in rivers_to_do[1:3]){ # test of several rivers
  # for(river in "AND__GSMACK_DSi"){ # test a particular river
  
  # Loop - Set Up Steps ----
  
  # Identify corresponding Stream_ID
  stream_id <- chemistry %>%
    dplyr::filter(Stream_Element_ID == river) %>%
    dplyr::select(Stream_ID) %>%
    unique() %>%
    as.character()
  
  # Also element
  element <- chemistry %>%
    dplyr::filter(Stream_Element_ID == river) %>%
    dplyr::select(variable) %>%
    unique() %>%
    as.character()
  
  # Subset chemistry
  river_chem <- chemistry %>%
    dplyr::filter(Stream_Element_ID == river) %>%
    # Drop unneeded columns
    dplyr::select(-Stream_Element_ID, -Stream_ID, -variable)
  
  # Subset discharge to correct river
  river_disc <- discharge %>%
    dplyr::filter(Stream_ID == stream_id) %>%
    dplyr::select(Date, Q)
  
  # Create a common prefix for all outputs from this run of the loop
  out_prefix <- paste0(stream_id, "_", element, "_") 
  
  # Message completion of loop
  message("Processing begun for '", element, "' at stream '", stream_id, "'")
  
  # Grab start time for processing
  start <- Sys.time()
  
  # Information also subsetted to right river
  river_info <- information %>%
    dplyr::filter(Stream_ID == stream_id) %>%
    # Generate correct information for this element
    dplyr::mutate(constitAbbrev = element) %>%
    dplyr::mutate(paramShortName = dplyr::case_when(
      constitAbbrev == "DSi" ~ "Silicon",
      constitAbbrev == "NOx" ~ "Nitrate_NOx",
      constitAbbrev == "NO3" ~ "Nitrate_NO3",
      constitAbbrev == "P" ~ "Phosphorous",
      constitAbbrev == "NH4" ~ "Ammonium",
      constitAbbrev == "TP" ~ "Total_Phosphorous",
      constitAbbrev == "TN" ~ "Total_Nitrogen")) %>%
    # Create another needed column
    dplyr::mutate(staAbbrev = shortName) %>%
    # Drop stream ID now that subsetting is complete
    dplyr::select(-Stream_ID)
  
  # Save these as CSVs with generic names
  ## This means each iteration of the loop will overwrite them so this folder won't become gigantic
  write.csv(x = river_disc, row.names = F, na = "",
            file = file.path(path, "WRTDS Temporary Files", "discharge.csv"))
  write.csv(x = river_chem, row.names = F, na = "",
            file = file.path(path, "WRTDS Temporary Files", "chemistry.csv"))
  write.csv(x = river_info, row.names = F, na = "",
            file = file.path(path, "WRTDS Temporary Files", "information.csv"))
  
  # Then read them back in with EGRET's special acquisition functions
  egret_disc <- EGRET::readUserDaily(filePath = file.path(path, "WRTDS Temporary Files"), fileName = "discharge.csv", qUnit = 2, verbose = F)
  egret_chem <- EGRET::readUserSample(filePath = file.path(path, "WRTDS Temporary Files"), fileName = "chemistry.csv", verbose = F)
  egret_info <- EGRET::readUserInfo(filePath = file.path(path, "WRTDS Temporary Files"), fileName = "information.csv", interactive = F)
  
  # Loop - Define Initial Objects ----
  
  # Create a list of the discharge, chemistry, and information files
  egret_list <- EGRET::mergeReport(INFO = egret_info, Daily = egret_disc, Sample = egret_chem, verbose = F)
  
  # Run series
  egret_list_out <- EGRET::runSeries(eList = egret_list, windowSide = 11, minNumObs = 50,
                                     verbose = F, windowS = 0.5)
  
  # Handle rivers that have blank time periods
  if(stream_id == "USGS__Mississippi River at Grafton"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1981-10-01", 
                                       endBlank = "1982-09-29") }
  if(stream_id == "USGS__PICEANCE CREEK RYAN GULCH"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1998-10-01",
                                       endBlank = "1999-09-30") }
  if(stream_id == "USGS__YAMPA RIVER AT DEERLODGE PARK"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1994-10-01",
                                       endBlank = "1996-09-30") }
  if(stream_id == "USGS__YUKON RIVER"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1996-10-01",
                                       endBlank = "2001-09-29") }
  
  # Loop - Period of Absence Tweaks ----
  
  # Some rivers just need the period of absence tweaked
  ## McMurdo (12 to 2)
  if(stringr::str_sub(string = stream_id, start = 1, end = 3) == "MCM"){
    egret_list <- EGRET::setPA(eList = egret_list, paStart = 12, paLong = 2)
    egret_list_out <- EGRET::setPA(eList = egret_list_out, paStart = 12, paLong = 2) }
  ## 5 to 5
  if(river %in% unique(pa5_5)){
    egret_list <- EGRET::setPA(eList = egret_list, paStart = 5, paLong = 5)
    egret_list_out <- EGRET::setPA(eList = egret_list_out, paStart = 5, paLong = 5) }
  ## 5 to 3
  if(river %in% unique(pa5_3)){
    egret_list <- EGRET::setPA(eList = egret_list, paStart = 5, paLong = 3)
    egret_list_out <- EGRET::setPA(eList = egret_list_out, paStart = 5, paLong = 3) }
  
  # Fit original model
  egret_estimation <- EGRET::modelEstimation(eList = egret_list, windowS = 0.5,
                                             minNumObs = 50, verbose = F)
  
  # Identify error statistics
  egret_error <- EGRET::errorStats(eList = egret_estimation)
  
  # Save the error stats out
  write.csv(x = egret_error, file = file.path(path, "WRTDS Outputs", paste0(out_prefix, "ErrorStats_WRTDS.csv")), row.names = F, na = "")
  
  # Calculate flux bias statistic
  flux_bias <- EGRET::fluxBiasStat(localSample = EGRET::getSample(x = egret_estimation))
  
  # Export that
  write.csv(x = flux_bias, file = file.path(path, "WRTDS Outputs", paste0(out_prefix, "FluxBias_WRTDS.csv")), row.names = F, na = "")
  
  # Create PDF report of preliminary graphs
  HERON::egret_report(eList_estim = egret_estimation, eList_series = egret_list_out,
                      out_path = file.path(path, "WRTDS Outputs", paste0(out_prefix, "WRTDS_GFN_output.pdf")))
  
  # Create annual averages
  egret_annual <- EGRET::tableResults(eList = egret_list_out)
  ## Can't silence this function... >:(
  
  # Export that as a CSV also
  write.csv(x = egret_annual, file.path(path, "WRTDS Outputs", paste0(out_prefix, "ResultsTable_GFN_WRTDS.csv")), row.names = F, na = "")
  
  # Identify monthly results
  egret_monthly <- EGRET::calculateMonthlyResults(eList = egret_list_out)
  
  # Export that
  write.csv(x = egret_monthly, file.path(path, "WRTDS Outputs", paste0(out_prefix, "Monthly_GFN_WRTDS.csv")), row.names = F, na = "")
  
  # Extract daily chemical value from run
  egret_concentration <- egret_list_out$Daily
  
  # Export that as well
  write.csv(x = egret_concentration, file.path(path, "WRTDS Outputs", paste0(out_prefix, "GFN_WRTDS.csv")), row.names = F, na = "")
  
  # Get flow normalized trends (flux and concentration)
  egret_flownorm <- HERON::egret_trends(eList_series = egret_list_out, flux_unit = 8)
  
  # Export it!
  write.csv(x = egret_flownorm, file.path(path, "WRTDS Outputs", paste0(out_prefix, "TrendsTable_GFN_WRTDS.csv")), row.names = F, na = "")
  
  # Grab the end processing time
  end <- Sys.time()
  
  # Combine timing into a dataframe
  loop_diagnostic <- data.frame("stream" = stream_id,
                                "chemical" = element,
                                "loop_start" = start,
                                "loop_end" = end)
  
  # Export this as well
  write.csv(x = loop_diagnostic, file.path(path, "WRTDS Loop Diagnostic", paste0(out_prefix, "Loop_Diagnostic.csv")), row.names = F, na = "")
  
  # Message completion of loop
  message("Processing complete for '", element, "' at stream '", stream_id, "'")
  
} # End loop

# End ----
