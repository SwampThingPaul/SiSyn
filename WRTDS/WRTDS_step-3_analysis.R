## ---------------------------------------------- ##
           # WRTDS Centralized Workflow
## ---------------------------------------------- ##
# WRTDS = Weighted Regressions on Time, Discharge, and Season
## Nick J Lyon

## ---------------------------------------------- ##
                # Housekeeping ----
## ---------------------------------------------- ##
# Load libraries
# install.packages("librarian")
librarian::shelf(tidyverse, googledrive, lubridate, EGRET, EGRETci, lter/HERON, supportR)

# Clear environment
rm(list = ls())

# If working on server, need to specify correct path
(path <- scicomptools::wd_loc(local = FALSE, remote_path = file.path('/', "home", "shares", "lter-si", "WRTDS")))

# Make sure necessary folders exist
dir.create(path = file.path(path, "WRTDS Inputs"), showWarnings = F)
dir.create(path = file.path(path, "WRTDS Temporary Files"), showWarnings = F)
dir.create(path = file.path(path, "WRTDS Loop Diagnostic"), showWarnings = F)
dir.create(path = file.path(path, "WRTDS Outputs"), showWarnings = F)

# Identify CSVs generated by 'step-2' script
input_ids <- googledrive::drive_ls(googledrive::as_id("https://drive.google.com/drive/u/0/folders/1QEofxLdbWWLwkOTzNRhI6aorg7-2S3JE"))

# Download them locally
purrr::walk2(.x = input_ids$name, .y = input_ids$id,
             .f = ~ googledrive::drive_download(file = .y, overwrite = T,
                                                path = file.path(path, "WRTDS Inputs", .x)))

# Read in CSVs generated by 'step-2' script
discharge <- read.csv(file.path(path, "WRTDS Inputs", "WRTDS-input_discharge.csv"))
chemistry <- read.csv(file.path(path, "WRTDS Inputs", "WRTDS-input_chemistry.csv"))
information <- read.csv(file.path(path, "WRTDS Inputs", "WRTDS-input_information.csv"))

## ---------------------------------------------- ##
            # Diagnose Types of Sites ----
## ---------------------------------------------- ##

# Rivers with warning that there are duplicated dates (should be impossible)
duplicate_data <- c(
  # Warning about "duplicated Daily dates" and "duplicated Sample dates"
  ### "Error in seq.Date(surfaceStart, by = "1 year", length.out = nSeg) : 
  ###'from' must be of length 1"
  # "USGS__Wild River_DSi", "USGS__Wild River_NH4", "USGS__Wild River_NOx", 
  # "USGS__Wild River_P"
  )

pa5_5 <- c(
  # EGRET::setPA(eList = egret_list[_out], paStart = 5, paLong = 5)
  "NWT__MARTINELLI_DSi", "NWT__MARTINELLI_NH4", 
  "NWT__MARTINELLI_NOx", "NWT__MARTINELLI_P")

pa5_3 <- c(
  # EGRET::setPA(eList = egret_list[_out], paStart = 5, paLong = 3)
  "NWT__SADDLE STREAM 007_DSi", "NWT__SADDLE STREAM 007_NH4", 
  "NWT__SADDLE STREAM 007_NOx", "NWT__SADDLE STREAM 007_P")

# Other odd errors
odd_ones <- c(
  # `EGRET::runSeries` issue:
  ## "Error in seq.Date(surfaceStart, by = "1 year", length.out = nSeg):
  ## 'from' must be of length 1"
  "GRO__Kolyma_DSi", "GRO__Kolyma_NH4", "GRO__Kolyma_NOx",
  "GRO__Mackenzie_DSi", "GRO__Mackenzie_NH4", "GRO__Mackenzie_NOx",
  
  # `EGRET::tableResults` issue:
  ## "Error in seq.default(xFirst, xLast) : 'from' must be a finite number"
  ### Looks like this may be caused by "negative flow days"?
  ### Need to check range of values to see these problem values
  "UMR__CH00.1M_NOx", "UMR__CH00.1M_P", "UMR__CN00.1M_P",
  "UMR__CU11.6M_DSi", "UMR__CU11.6M_NOx", "UMR__CU11.6M_P",
  "UMR__LM00.5M_NOx", "UMR__LM00.5M_P", "UMR__M078.0B_NOx",
  "UMR__M241.4K_NOx", "UMR__M241.4K_P", "UMR__M556.4A_NOx",
  "UMR__M556.4A_P", "UMR__M701.1B_NOx", "UMR__M701.1B_P",
  "UMR__M764.3A_NOx", "UMR__M764.3A_P", "UMR__M786.2C_P",
  "UMR__MQ02.1M_NOx", "UMR__MQ02.1M_P", "UMR__SG16.2C_NOx",
  "UMR__SG16.2C_P", "UMR__WP02.6M_DSi", "UMR__WP02.6M_NOx", 
  "UMR__WP02.6M_P", 
  
  # `EGRET::tableResults` issue:
  ## "Error in if (good) { : missing value where TRUE/FALSE needed"
  "HYBAM__Manacapuru_DSi", "HYBAM__Manacapuru_NO3", 
  "MD__Barr Creek_NOx", "MD__Barr Creek_P",
  "UK__BURE AT HORSTEAD MILL_DSi",
  
  # `EGRET::tableResults` issue:
  ## "Error in if (lastMonth == 2 & (lastYear%%4 == 0) & ((lastYear%%100 != : 
  ## missing value where TRUE/FALSE needed"
  "Australia__DARLING RIVER AT BOURKE TOWN_NO3",
  "Australia__DARLING RIVER AT BURTUNDY_NO3",
  "Australia__DARLING RIVER AT BURTUNDY_NOx",
  "Australia__DARLING RIVER AT BURTUNDY_P",
  "Australia__DARLING RIVER AT WILCANNIA MAIN CHANNEL_NO3",
  "Australia__EDWARD RIVER AT MOULAMEIN_NOx",
  "Cameroon__Nsimi_outlet_DSi", "Cameroon__Nsimi_outlet_NO3",
  "HYBAM__Borja_DSi", "HYBAM__Itaituba_DSi", "HYBAM__Itaituba_NO3",
  "HYBAM__Langa Tabiki_DSi", "HYBAM__Langa Tabiki_NO3", 
  "HYBAM__Saut Maripa_DSi", "HYBAM__Saut Maripa_NO3", 
  "LUQ__RI_DSi", "LUQ__RI_NH4", "LUQ__RI_NOx", "LUQ__RI_P", 
  "MD__Broken Creek_NOx", "MD__Broken Creek_P", "MD__Gunbower Creek_NOx", 
  "MD__Gunbower Creek_P", "MD__Lock 5_NOx", "MD__Lock 5_P",
  "UK__ALT AT KIRKBY_DSi", "UK__ARUN AT PALLINGHAM_DSi", "UK__AVON AT BATHFORD_DSi",
  "UK__AVON AT LODDISWELL_DSi", "UK__CALDER AT WHALLEY WEIR_DSi",
  "UK__CUCKMERE AT SHERMAN BRIDGE_DSi", "UK__DEARNE AT ADWICK_DSi",
  "UK__DON AT DONCASTER_DSi"
)

# Rivers without sufficient data
few_data <- c(
  # `EGRET::errorStats` issue:
  ## "Error in runSurvReg(SampleCrossV$DecYear[i], SampleCrossV$LogQ[i], DecLow,:
  ## minNumUncen is greater than total number of samples"
  "AND__GSWS06_NOx", "AND__GSWS07_NOx",
  "Australia__BARWON RIVER AT DANGAR BRIDGE WALGETT_NOx",
  "Australia__BARWON RIVER AT DANGAR BRIDGE WALGETT_P",
  "Australia__DARLING RIVER AT BOURKE TOWN_NH4",
  "Australia__DARLING RIVER AT BOURKE TOWN_NOx",
  "Australia__DARLING RIVER AT BOURKE TOWN_P",
  "Australia__DARLING RIVER AT WILCANNIA MAIN CHANNEL_NH4",
  "Australia__DARLING RIVER AT WILCANNIA MAIN CHANNEL_NOx",
  "Australia__DARLING RIVER AT WILCANNIA MAIN CHANNEL_P",
  "Australia__EDWARD RIVER AT DENILIQUIN_NH4",
  "Australia__EDWARD RIVER AT DENILIQUIN_NO3",
  "Australia__NARRABRI CREEK AT NARRABRI_NH4",
  "Australia__PEEL RIVER AT UPSTREAM PARADISE WEIR_P",
  "GRO__Yenisey_P", "GRO__Kolyma_P", "GRO__Lena_P", 
  "GRO__Mackenzie_P", "GRO__Yukon_P",
  "HBR__ws1_P", "HBR__ws2_P", "HBR__ws3_P", 
  "HBR__ws4_P", "HBR__ws5_P", "HBR__ws6_P",
  "HBR__ws7_P", "HBR__ws8_P", "HBR__ws9_P",
  "HYBAM__Atalaya Aval_NO3", "HYBAM__Manacapuru_NOx", "HYBAM__Manacapuru_P", 
  "HYBAM__Nazareth_DSi", "HYBAM__Nazareth_NO3", "HYBAM__Obidos_NOx",
  "HYBAM__Obidos_P",
  "MCM__Canada Stream at F1_P", "MCM__Onyx River at Lake Vanda Weir_NH4",
  "MCM__Onyx River at Lake Vanda Weir_P", "MCM__Onyx River at Lower Wright Weir_NH4",
  "MCM__Onyx River at Lower Wright Weir_P", "MCM__Priscu Stream at B1_NH4",
  "MCM__Von Guerard Stream at F6_NH4", 
  "NIVA__BUSEDRA_NH4", "NIVA__BUSEDRA_P", "NIVA__FINEALT_NH4", "NIVA__FINEALT_P",
  "NIVA__NOREVEF_NH4", "NIVA__NOREVEF_P", "NIVA__OSTEGLO_NH4", "NIVA__ROGEORR_NH4",
  "NIVA__STREORK_NH4", "NIVA__STREORK_P", "NIVA__TELESKI_NH4", "NIVA__TELESKI_P",
  "NIVA__VAGEOTR_NH4", "NIVA__VAGEOTR_P", "NIVA__VESENUM_NH4", "NIVA__VESENUM_P",
  "NWT__ALBION_NOx", "NWT__ALBION_P", "NWT__MARTINELLI_P", 
  "NWT__SADDLE STREAM 007_NOx", "NWT__SADDLE STREAM 007_P", 
  "Sagehen__Sagehen_NH4", 
  
  # Error in `EGRET::runSeries`
  ## "Error in runSurvReg(estPtYear, estPtLogQ, DecLow, DecHigh, localSample,:
  ## minNumObs is greater than total number of samples"
  "Australia__BARWON RIVER AT MUNGINDI_NO3", "Australia__BARWON RIVER AT MUNGINDI_P",
  "Australia__BILLABONG CREEK AT DARLOT_NO3", "Australia__BILLABONG CREEK AT DARLOT_NOx",
  "Australia__BILLABONG CREEK AT DARLOT_P", "Australia__DARLING RIVER AT BURTUNDY_NH4",
  "Australia__DARLING RIVER AT MENINDEE UPSTREAM WEIR 32_NH4",
  "Australia__EDWARD RIVER AT DENILIQUIN_P", "Australia__EDWARD RIVER AT MOULAMEIN_P",
  "Australia__MURRAY RIVER DOWNSTREAM YARRAWONGA WEIR_NH4",
  "Australia__NAMOI RIVER AT GOANGRA_NH4", "Australia__NAMOI RIVER AT GOANGRA_NOx",
  "Australia__NAMOI RIVER AT GOANGRA_P", "Australia__NARRABRI CREEK AT NARRABRI_NOx",
  "Australia__NARRABRI CREEK AT NARRABRI_P", 
  "Australia__PEEL RIVER AT UPSTREAM PARADISE WEIR_NH4",
  "HYBAM__Borja_NO3", "HYBAM__Ciudad Bolivar_NO3",
  "NIVA__AAGEVEG_DSi", "NIVA__FINEPAS_DSi", "NIVA__FINETAN_DSi", "NIVA__HOREVOS_DSi",
  "NIVA__MROEDRI_DSi", "NIVA__OSLEALN_DSi", "NIVA__ROGEBJE_DSi", "NIVA__ROGEVIK_DSi",
  "NIVA__SFJENAU_DSi", "NIVA__STRENID_DSi",
  "UK__DERWENT AT SEATON MILL_DSi"
  )

# Rivers that crash R without a specific error message
crash_rivers <- c(
  "Australia__EDWARD RIVER AT DENILIQUIN_NOx",
  "Australia__PEEL RIVER AT UPSTREAM PARADISE WEIR_NO3",
  
  "USGS__GORE CREEK UPPER STATION_NH4", "USGS__GORE CREEK UPPER STATION_P",
  "USGS__YUKON RIVER_P"
  
)

# Set of problem rivers to drop from the loop (for reasons not specified above)
bad_rivers <- c(
  # River names with slashes cause a file path issue later on
  "UK__EDEN AT PENSHURST / VEXOUR BRIDGE_DSi", 
  "UK__MEDWAY AT TESTON / EAST FARLEIGH_DSi")

# Identify all rivers that aren't in the broken data vectors
good_rivers <- setdiff(x = unique(chemistry$Stream_Element_ID),
                       y = unique(c(few_data, duplicate_data, 
                                    odd_ones, bad_rivers, crash_rivers)))
## Note this includes weird rivers that need special treatment and those that don't

## ---------------------------------------------- ##
              # Analysis Workflow ----
## ---------------------------------------------- ##

# Vector for storing problem rivers identified in latest run of WRTDS
new_bads <- c()

# Set of rivers we've already run the workflow for
done_rivers <- data.frame("file" = dir(path = file.path(path, "WRTDS Loop Diagnostic"))) %>%
  # Drop the file suffix part of the file name 
  dplyr::mutate(river = gsub(pattern = "\\_Loop\\_Diagnostic.csv", replacement = "", x = file))

# Identify rivers to run
rivers_to_do <- sort(setdiff(x = unique(good_rivers), 
                             y = c(unique(done_rivers$river), new_bads)))

# What are the next few that will be processed and how many total left?
rivers_to_do[1:5]; length(rivers_to_do)

# Loop across rivers and elements to run WRTDS workflow!
for(river in rivers_to_do){ # actual loop
  # for(river in rivers_to_do[1:3]){ # test of several rivers
  # for(river in "AND__GSMACK_DSi"){ # test a particular river
  
  # Loop - Set Up Steps ----
  
  # Identify corresponding Stream_ID
  stream_id <- chemistry %>%
    dplyr::filter(Stream_Element_ID == river) %>%
    dplyr::select(Stream_ID) %>%
    unique() %>%
    as.character()
  
  # Also element
  element <- chemistry %>%
    dplyr::filter(Stream_Element_ID == river) %>%
    dplyr::select(variable) %>%
    unique() %>%
    as.character()
  
  # Subset chemistry
  river_chem <- chemistry %>%
    dplyr::filter(Stream_Element_ID == river) %>%
    # Drop unneeded columns
    dplyr::select(-Stream_Element_ID, -Stream_ID, -variable)
  
  # Subset discharge to correct river
  river_disc <- discharge %>%
    dplyr::filter(Stream_ID == stream_id) %>%
    dplyr::select(Date, Q)
  
  # Create a common prefix for all outputs from this run of the loop
  out_prefix <- paste0(stream_id, "_", element, "_") 
  
  # Message completion of loop
  message("Processing begun for '", river, "'")
  
  # Grab start time for processing
  start <- Sys.time()
  
  # Information also subsetted to right river
  river_info <- information %>%
    dplyr::filter(Stream_ID == stream_id) %>%
    # Generate correct information for this element
    dplyr::mutate(constitAbbrev = element) %>%
    dplyr::mutate(paramShortName = dplyr::case_when(
      constitAbbrev == "DSi" ~ "Silicon",
      constitAbbrev == "NOx" ~ "Nitrate_NOx",
      constitAbbrev == "NO3" ~ "Nitrate_NO3",
      constitAbbrev == "P" ~ "Phosphorous",
      constitAbbrev == "NH4" ~ "Ammonium",
      constitAbbrev == "TP" ~ "Total_Phosphorous",
      constitAbbrev == "TN" ~ "Total_Nitrogen")) %>%
    # Create another needed column
    dplyr::mutate(staAbbrev = shortName) %>%
    # Drop stream ID now that subsetting is complete
    dplyr::select(-Stream_ID)
  
  # Save these as CSVs with generic names
  ## This means each iteration of the loop will overwrite them so this folder won't become gigantic
  write.csv(x = river_disc, row.names = F, na = "",
            file = file.path(path, "WRTDS Temporary Files", "discharge.csv"))
  write.csv(x = river_chem, row.names = F, na = "",
            file = file.path(path, "WRTDS Temporary Files", "chemistry.csv"))
  write.csv(x = river_info, row.names = F, na = "",
            file = file.path(path, "WRTDS Temporary Files", "information.csv"))
  
  # Then read them back in with EGRET's special acquisition functions
  egret_disc <- EGRET::readUserDaily(filePath = file.path(path, "WRTDS Temporary Files"), fileName = "discharge.csv", qUnit = 2, verbose = F)
  egret_chem <- EGRET::readUserSample(filePath = file.path(path, "WRTDS Temporary Files"), fileName = "chemistry.csv", verbose = F)
  egret_info <- EGRET::readUserInfo(filePath = file.path(path, "WRTDS Temporary Files"), fileName = "information.csv", interactive = F)
  
  # Loop - Define Initial Objects ----
  
  # Create a list of the discharge, chemistry, and information files
  egret_list <- EGRET::mergeReport(INFO = egret_info, Daily = egret_disc, Sample = egret_chem, verbose = F)
  
  # Run series
  egret_list_out <- EGRET::runSeries(eList = egret_list, windowSide = 11, minNumObs = 50,
                                     verbose = F, windowS = 0.5)
  
  # Handle rivers that have blank time periods
  if(stream_id == "USGS__Mississippi River at Grafton"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1981-10-01", 
                                       endBlank = "1982-09-29") }
  if(stream_id == "USGS__PICEANCE CREEK RYAN GULCH"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1998-10-01",
                                       endBlank = "1999-09-30") }
  if(stream_id == "USGS__YAMPA RIVER AT DEERLODGE PARK"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1994-10-01",
                                       endBlank = "1996-09-30") }
  if(stream_id == "USGS__YUKON RIVER"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1996-10-01",
                                       endBlank = "2001-09-29") }
  
  # Loop - Period of Absence Tweaks ----
  
  # Some rivers just need the period of absence tweaked
  ## McMurdo (12 to 2)
  if(stringr::str_sub(string = stream_id, start = 1, end = 3) == "MCM"){
    egret_list <- EGRET::setPA(eList = egret_list, paStart = 12, paLong = 2)
    egret_list_out <- EGRET::setPA(eList = egret_list_out, paStart = 12, paLong = 2) }
  ## 5 to 5
  if(river %in% unique(pa5_5)){
    egret_list <- EGRET::setPA(eList = egret_list, paStart = 5, paLong = 5)
    egret_list_out <- EGRET::setPA(eList = egret_list_out, paStart = 5, paLong = 5) }
  ## 5 to 3
  if(river %in% unique(pa5_3)){
    egret_list <- EGRET::setPA(eList = egret_list, paStart = 5, paLong = 3)
    egret_list_out <- EGRET::setPA(eList = egret_list_out, paStart = 5, paLong = 3) }
  
  # Fit original model
  egret_estimation <- EGRET::modelEstimation(eList = egret_list, windowS = 0.5,
                                             minNumObs = 50, verbose = F)
  
  # Fit WRTDS Kalman
  egret_kalman <- EGRET::WRTDSKalman(eList = egret_estimation, niter = 200, verbose = T)
  
  # Identify error statistics
  # egret_error <- EGRET::errorStats(eList = egret_estimation)
  egret_error <- EGRET::errorStats(eList = egret_kalman)
  
  # Save the error stats out
  write.csv(x = egret_error, file = file.path(path, "WRTDS Outputs", paste0(out_prefix, "ErrorStats_WRTDS.csv")), row.names = F, na = "")
  
  # Calculate flux bias statistic
  # flux_bias <- EGRET::fluxBiasStat(localSample = EGRET::getSample(x = egret_estimation))
  flux_bias <- EGRET::fluxBiasStat(localSample = EGRET::getSample(x = egret_kalman))
  
  # Export that
  write.csv(x = flux_bias, file = file.path(path, "WRTDS Outputs", paste0(out_prefix, "FluxBias_WRTDS.csv")), row.names = F, na = "")
  
  # Create PDF report of preliminary graphs
  HERON::egret_report(eList_estim = egret_estimation, eList_series = egret_list_out,
                      out_path = file.path(path, "WRTDS Outputs", paste0(out_prefix, "WRTDS_GFN_output.pdf")))
  
  # Create annual averages
  egret_annual <- EGRET::tableResults(eList = egret_list_out)
  egret_annual_kalman <- EGRET::setupYears(localDaily = egret_kalman$Daily)
  
  # Export both as CSVs also
  write.csv(x = egret_annual, file.path(path, "WRTDS Outputs", paste0(out_prefix, "ResultsTable_GFN_WRTDS.csv")), row.names = F, na = "")
  write.csv(x = egret_annual_kalman, file.path(path, "WRTDS Outputs", paste0(out_prefix, "ResultsTable_Kalman_WRTDS.csv")), row.names = F, na = "")
  
  # Identify monthly results
  egret_monthly <- EGRET::calculateMonthlyResults(eList = egret_list_out)
  egret_monthly_kalman <- EGRET::calculateMonthlyResults(eList = egret_kalman)
  
  # Export those
  write.csv(x = egret_monthly, file.path(path, "WRTDS Outputs", paste0(out_prefix, "Monthly_GFN_WRTDS.csv")), row.names = F, na = "")
  write.csv(x = egret_monthly_kalman, file.path(path, "WRTDS Outputs", paste0(out_prefix, "Monthly_Kalman_WRTDS.csv")), row.names = F, na = "")
  
  # Extract daily chemical value from run
  egret_concentration <- egret_list_out$Daily
  egret_conc_kalman <- egret_kalman$Daily
  
  # Export those as well
  write.csv(x = egret_concentration, file.path(path, "WRTDS Outputs", paste0(out_prefix, "GFN_WRTDS.csv")), row.names = F, na = "")
  write.csv(x = egret_conc_kalman, file.path(path, "WRTDS Outputs", paste0(out_prefix, "Kalman_WRTDS.csv")), row.names = F, na = "")
  
  # Get flow normalized trends (flux and concentration)
  egret_flownorm <- HERON::egret_trends(eList_series = egret_list_out, flux_unit = 8)
  
  # Export it!
  write.csv(x = egret_flownorm, file.path(path, "WRTDS Outputs", paste0(out_prefix, "TrendsTable_GFN_WRTDS.csv")), row.names = F, na = "")
  
  # Grab the end processing time
  end <- Sys.time()
  
  # Combine timing into a dataframe
  loop_diagnostic <- data.frame("stream" = stream_id,
                                "chemical" = element,
                                "loop_start" = start,
                                "loop_end" = end)
  
  # Export this as well
  write.csv(x = loop_diagnostic, file.path(path, "WRTDS Loop Diagnostic", paste0(out_prefix, "Loop_Diagnostic.csv")), row.names = F, na = "")
  
  # Message completion of loop
  message("Processing complete for '", river, "'")
  
  # Remove all objects created inside loop
  ## This makes figuring out where the loop breaks *much* easier!
  rm(list = c("stream_id", "element", "river_chem", "river_disc", "out_prefix", 
              "start", "river_info", "egret_disc", "egret_chem", "egret_info", 
              "egret_list", "egret_list_out", "egret_estimation", "egret_error", 
              "flux_bias", "egret_annual", "egret_annual_kalman", 
              "egret_monthly", "egret_monthly_kalman", "egret_concentration", 
              "egret_conc_kalman", "egret_flownorm", "end", "loop_diagnostic"))
  
} # End loop

# Object creation order (for error diagnostics):
## 0. stream_id; element; river_chem; river_disc; out_prefix; start; river_info
## 1. egret_disc; egret_chem; egret_info -- `EGRET::readUser___`
## 2. egret_list; egret_list_out -- `EGRET::mergeReport` & `EGRET::runSeries`
## 3. egret_estimation -- `EGRET::modelEstimation`
## 4. egret_kalman -- `EGRET::WRTDSKalman`
## 5. egret_error -- `EGRET::errorStats`
## 6. flux_bias -- `EGRET::fluxBiasStat`
## 7A. egret_annual -- `EGRET::tableResults`
## 7B. egret_annual_kalman -- `EGRET::tableResults`
## 8. egret_monthly; egret_monthly_kalman -- `EGRET::calculateMonthlyResults`
## 9. egret_concentration; egret_conc_kalman
## 10. egret_flownorm -- `HERON::egret_trends`
## 11. end; loop_diagnostic

# End ----
