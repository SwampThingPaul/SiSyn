## Code to Run WRTDS model using EGRET and EGRETci packages
# written by Keira Johnson Nov 2020
# modified by Kathi Jo Jankowski April 2021

### downloads model "prep" files from google drive that were generated by "WRTDS_prep" code
### runs model for all sites in a loop - some sites need special conditions - see code
### runs two trend tests and saves results - tableChange and EGRETci
### saves model results: daily estimates, annual estimates, model diagnostics

## Load packages
#install.packages("googledrive")
#install.packages("tidyverse")
#install.packages("data.table)
require(tidyverse)
require(googledrive)
require(stringr)
require(lubridate)
require(reshape)
require(gtools)
require(data.table)
require(dataRetrieval)
require(EGRET)
require(EGRETci)

## Get files from google drive
#get folder URL from google drive with Prep files - CHANGE for P files!
WRTDSfolder_url<-"https://drive.google.com/drive/folders/1s6irhuhH3qTdEi8gHRu5COFuHZyz4T9d?usp=sharing"

#get ID of folder
WRTDSfolder<-drive_get(as_id(WRTDSfolder_url))

#get list of csv files from folder
WRTDScsv_files<-drive_ls(WRTDSfolder, type="csv")

#extract only WRTDS files - there are some regular discharge files in the WRTDScsv_files dataframe
WRTDScsv_files_final<-WRTDScsv_files[WRTDScsv_files$name %like% "WRTDS.csv",]

#create new column of just the site
WRTDScsv_files_final$files<-word(WRTDScsv_files_final$name, 1, sep = "_")

## Read in file that identifies sites for long-term analysis
# stored here: https://drive.google.com/file/d/15sgOJvSswiVXtWVl_S-QwDRmQyrI_pyU/view?usp=sharing
#setwd("/Users/keirajohnson/Box Sync/Keira_Johnson/SiSyn")
setwd("U:/Jankowski/My Documents/Projects/Silica Synthesis/Data/Chem Data")

#read in file of 20 year data
data_20yearQ<-read.csv("Data_years_streams_WRTDS.csv")

#filter out to get daily data
#data_20yearQ_daily<-subset(data_20yearQ, data_20yearQ$Q.Data.type=="daily")

#create list of unique sites in this LTER
files20years<-unique(data_20yearQ$Stream.Site)

#filter the original csv file to include only sites for analysis
RefTable<-subset(WRTDScsv_files_final, WRTDScsv_files_final$files %in% files20years)

#create list of unique files/sites
site_files<-unique(RefTable$name)

# for some reason "GSWS09_Si_WRTDS" was listed twice in google drive and wouldn't download, skipped it and did it manually..
site_files = site_files[1:length(site_files)]

## Set wd to new folder where these files will be downloaded - WRTDS requires them to be stored locally
#setwd("/Users/keirajohnson/Box Sync/Keira_Johnson/SiSyn/WRTDS_20years")
setwd("C:/Users/kjankowski/Desktop/WRTDS_prep")

#download each file to the above specified folder
for (i in 1:length(site_files)) {
  
  drive_download(site_files[i], type = "csv", overwrite = TRUE)
  
}

## Create lists for loop
#make list of all files that you just downloaded to local folder
WRTDS_files_List<-list.files(path = "C:/Users/kjankowski/Desktop/WRTDS_prep")

#make list of only Q files
WRTDS_files_List_Q<-WRTDS_files_List[WRTDS_files_List %like% "Q_WRTDS"]

#make list of only P files
WRTDS_files_List_P<-WRTDS_files_List[WRTDS_files_List %like% "P"]

#make list of only INFO files
WRTDS_files_List_Info<-WRTDS_files_List[WRTDS_files_List %like% "INFO"]

WRTDS_files<-sub("*_Q_WRTDS.csv", "", WRTDS_files_List_Q)

# for subsetting to test loop
# WRTDS_files = WRTDS_files[1]

## Run WRTDS output will be saved to the same file
for (i in 1:length(WRTDS_files)) {
  
  #read in Q file
  Daily<-readUserDaily("C:/Users/kjankowski/Desktop/WRTDS_prep", WRTDS_files_List_Q[1],
                       qUnit = 2)
  
  #read in P file
  Sample<-readUserSample("C:/Users/kjankowski/Desktop/WRTDS_prep", WRTDS_files_List_P[1])
  
  #read in Info file
  Info<-readUserInfo("C:/Users/kjankowski/Desktop/WRTDS_prep", WRTDS_files_List_Info[1])
  
  #remove duplicates from sample file
  Sample<-removeDuplicates(Sample)
  
  Sample<-aggregate(Sample, by=list(Sample$Date), mean)
  
  Sample<-Sample[,-c(1)]
  
  #merge into eList
  eList<-mergeReport(Info, Daily, Sample)
  
  #save workspace so it can be accessed later
  savePath<-"C:/Users/kjankowski/Desktop/WRTDSWorkspaces/"
  saveResults(savePath, eList)
  
  #estimate continuous P
  eList<-modelEstimation(eList, minNumObs=50)
  
  ## For Sagehen Site!
  eList <- blankTime(eList, startBlank = "1996-01-01", endBlank = "2001-01-01")
  
  ## Needs to be adjusted for MCM and NWT sites!
  eList <- setPA(eList, paStart=5, paLong=5)
  
  #write output to this folder
  #setwd("/Users/keirajohnson/Box Sync/Keira_Johnson/SiSyn/WRTDSfinal")
  setwd("C:/Users/kjankowski/Desktop/WRTDSResults")
  
  
  #extract continuous P file from eList
  ContConc<-eList$Daily
  
  #write csv of continuous P data
  write.csv(ContConc, paste0(WRTDS_files[i], "ContP_WRTDS.csv"))
  
  #average yearly stats
  Results<-tableResults(eList)
  
  #write csv of results dataframe
  write.csv(Results, paste0(WRTDS_files[i], "ResultsTable_WRTDS.csv"))
  
  #make new column for year
  ContConc$Year<-format(as.Date(ContConc$Date), "%Y")
  
  #find min year
  minYP<-as.numeric(min(ContConc$Year))+1
  
  #find max year
  maxYP<-as.numeric(max(ContConc$Year))-1
  
  #set year points for 
  yearPoints<-c(minYP, maxYP)
  
  #calculate concentration trend
  Conc<-tableChangeSingle(eList, fluxUnit = 8, yearPoints)
  
  #calculate flux trend
  Flux<-tableChangeSingle(eList, fluxUnit = 8, yearPoints, flux = TRUE)
  
  #bind into one dataframe
  Trends<-cbind(Conc, Flux)
  
  #write csv of trends dataframe
  write.csv(Trends, paste0(WRTDS_files[i], "TrendsTable_WRTDS.csv"))
  
  ## EGRETCi Trends - takes a long time to run - generates boostrapped confidence intervals
  caseSetUp <- trendSetUp(eList, 
                          year1=minYP,
                          year2=maxYP,
                          nBoot = 100, 
                          bootBreak = 50,
                          blockLength = 200)
  eBoot<-wBT(eList, caseSetUp=caseSetUp)
  bootResults <- cbind(eBoot$xConc, eBoot$xFlux, eBoot$pConc, eBoot$pFlux)
  bootSummary <- eBoot$bootOut
  
  ## keep bootstrap results
  CIs=as.data.frame(bootResults)
  CIs$solute=rep("TN", nrow(bootResults))
  colnames(CIs)=c("xConc", "xFlux", "pConc", "pFlux", "solute")
  write.csv(CIs, paste0(WRTDS_files[i], "_EGRETCi_bootstraps.csv"), row.names=FALSE)
  
  # saves one line summary of trend info from EGRETci
  Summary=as.data.frame(bootSummary)
  write.csv(Summary, paste0(WRTDS_files[i], "_P_EGRETCi_Trend.csv"))
  
  #open pdf for graphical output
  pdf(paste0(WRTDS_files[i], "_WRTDS_output.pdf"))
  
  #residual plots
  fluxBiasMulti(eList)
  
  #examine model fit
  plotConcTimeDaily(eList)
  
  #plot concentration
  plotConcHist(eList) # minYP, maxYP)
  
  #plot flux
  plotFluxHist(eList) #, minYP, maxYP)
  
  #plot data
  multiPlotDataOverview(eList)

  
  dev.off()
  
}

